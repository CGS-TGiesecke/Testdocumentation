
==== Allgemeine Vorrausetzungen

|===
| *Infrastruktur (VM oder Server)*				| Linux-basierter Server (lokale VM oder Cloud-VM)
| *Betriebssystem*								| Linux, z. B. Ubuntu oder vergleichbare Distributionen
| *Docker-Umgebung*								| Docker Engine: Version ≥ 20.x 
|												| Docker Compose: Version ≥ 1.29 oder Compose V2
| *CPU, RAM, Speicher*							| Abhängig von der Größe der Dokumentensammlung und der Zahl der Benutzer
| *Gemeinsames Verzeichnis* *(SHARED_FOLDER)*	| Ein beschreibbares Verzeichnis auf dem Server für den Datenaustausch
| *Internetzugang / Proxy*						| Download der Docker-Images
|												| Verbindung zur Azure OpenAI API
|												| Proxy-Unterstützung ist möglich
| *Netzwerk / Ports*							| Standardzugang: Port 8000 (bei Bedarf anpassbar)
|===

===== Infrastruktur (VM oder Server)

Die Lösung wird auf einem **Linux-basierten Server** bereitgestellt (lokale VM oder Cloud-VM). Es ist *keine spezielle Cloud-Integration erforderlich* 
– jeder Linux-Server mit Docker Compose ist geeignet.

*Wichtig:* Eine VM ist *nicht zwingend erforderlich*. Die Lösung kann auf jedem Linux-Server mit Docker Compose betrieben werden.

===== Betriebssystem

- *Linux*, z. B. Ubuntu Server LTS (24.04) oder vergleichbare Distributionen
- Getestet mit Ubuntu; andere Linux-Distributionen auf Anfrage

===== Docker-Umgebung

Die Bereitstellung erfolgt über *Docker Compose*.

*Erforderliche Versionen:*

- *Docker Engine:* Version `>` 20.x
- *Docker Compose:* Version `>` 1.29 oder Compose V2

*Installation:* Der Kunde muss Docker und Docker Compose auf dem Server installieren und konfigurieren.

===== Gemeinsames Verzeichnis (SHARED_FOLDER)

Ein *beschreibbares Verzeichnis* auf dem Server für den Datenaustausch zwischen den Containern und für persistente Daten (Dokumente, Datenbanken, Vector-Store).

*Aufgabe des Kunden:* Bereitstellung eines Verzeichnisses mit ausreichenden Schreib-/Leserechten für die Docker-Container.

===== Internetzugang / Proxy

*Erforderlich für:*
- Download der Docker-Images (während der Installation)
- Verbindung zum LLM-Provider API (Azure OpenAI, AWS Bedrock - falls Cloud-Provider gewählt wird)

*Proxy-Unterstützung:* Die Lösung unterstützt den Betrieb hinter einem Proxy. Proxy-Konfiguration kann in den Umgebungsvariablen hinterlegt werden.

*Wichtig:* 
- *Bei Cloud-LLM-Providern (Azure OpenAI, AWS Bedrock):* Dauerhafte Internetverbindung erforderlich
- *Bei lokalen Modellen:* Nur für Installation erforderlich (Download der Docker-Images)

*Proxy-Unterstützung:* Die Lösung unterstützt den Betrieb hinter einem Proxy. Proxy-Konfiguration kann in den Umgebungsvariablen hinterlegt werden.

*MCP Server API:* Auf Wunsch kundenspezifische Erweiterung der Suchfunktionen für Web-, lokale Kunden-, Bild-, Video- und Nachrichtensuchen sowie KI gestützte Zusammenfassungen 

ifeval::[{big-output} > 1]
include::chapter2-2-o.adoc[]
endif::[]




