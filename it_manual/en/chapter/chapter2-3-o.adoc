==== Supported LLM Providers (Provisioned by the Customer)

**IMPORTANT:** {application} does not provide an LLM. The customer must set up and operate one of the following LLM providers themselves.

===== Option A: Azure OpenAI (Recommended)

**Why Azure OpenAI is recommended:**

- Proven enterprise integration
- Availability in EU regions (GDPR compliant)
- Microsoft enterprise support
- Reliable GPT models

**Requirements:**

- Active Azure subscription
- Approval for Azure OpenAI Service (may require approval)
- Sufficient permissions to create resources

===== Step-by-Step: Setting up Azure OpenAI Service

====== Step 1: Create Azure OpenAI Resource

1. **Open Azure Portal:** [https://portal.azure.com](https://portal.azure.com)
2. **Create resource:**
   - Click "+ Create a resource"
   - Search for "Azure OpenAI"
   - Click "Create"
3. **Resource configuration:**
   - **Subscription:** Select your Azure subscription
   - **Resource group:** Create a new or use existing
   - **Region:** Choose a region (e.g., West Europe, North Europe)
   - **Name:** Assign a unique name (e.g., `xxx-assist-openai-prod`)
   - **Pricing tier:** Standard S0 or higher
4. **Review and create:** Click "Review + Create" then "Create"

====== Step 2: Deploy Models in Azure AI Foundry

1. **Open Azure AI Foundry:** [https://ai.azure.com](https://ai.azure.com)
2. **Model deployment:**
   - Navigate to "Deployments"
   - Click "+ Create new deployment"
3. **Recommended models:**
   - **Main model:** `gpt-4o` 
   - **Optional:** `gpt-5` or higher 

====== Step 3: Get API Credentials

1. In Azure Portal → Your OpenAI Resource → "Keys and Endpoint"
2. **Note for later configuration in the application:**
   - API key
   - Endpoint (e.g., `https://your-resource-name.openai.azure.com/`)
   - Region (e.g., `westeurope`)
   - Deployment name of the models

**Example:**

```
Endpoint: https://xxx-assist-openai-prod.openai.azure.com/
API Key: 1234567890abcdef...
Region: westeurope
Deployment Name: gpt-4o
```

===== Option B: AWS Bedrock (Equally Supported)

**AWS Bedrock is a full-featured alternative to Azure OpenAI.**

**Advantages:**

- Excellent Claude models from Anthropic
- Availability in EU regions (Frankfurt, Ireland)
- Good integration for AWS customers
- Often more cost-effective than Azure OpenAI

**Requirements:**

- Active AWS account
- Access to the AWS Bedrock Service
- Sufficient IAM permissions

===== Step-by-Step: Setting up AWS Bedrock

====== Step 1: Enable AWS Bedrock

1. **Open AWS Console:** [https://console.aws.amazon.com](https://console.aws.amazon.com)
2. **Navigate to Bedrock:** Search for "Bedrock"
3. **Select region:** e.g., eu-central-1 (Frankfurt), us-east-1
4. **Enable Model Access:**
   - Navigate to "Model access"
   - Click on "Manage model access"
   - Enable Anthropic Claude models
   - Click on "Save changes"

====== Step 2: Set up IAM permissions

Create an IAM user with the following policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": "arn:aws:bedrock:*::foundation-model/*"
    }
  ]
}
```

====== Step 3: Generate Access Keys

1. IAM Console → Users → Your user → Security credentials
2. Create access key
3. **Note for later configuration in the application:**
   - AWS Access Key ID
   - AWS Secret Access Key
   - AWS Region
   - Model ID (e.g., `anthropic.claude-3-5-sonnet-20240620-v1:0`)

**Example:**

```bash
AWS Access Key ID: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key: wJalrXUtnFEMI/K7MDENG...
AWS Region: eu-central-1
Model ID: anthropic.claude-3-5-sonnet-20240620-v1:0
```

===== Option C: Local Models (for Highest Data Protection Requirements)

**For customers who do not want any cloud connection.**

The platform supports local LLMs via **OpenAI-compatible APIs**.

**Supported tools:**

- **Ollama** (the simplest solution) – https://ollama.ai
- **vLLM** (highest performance)
- **LM Studio** (GUI-based)

**Recommended models:**

- **Llama 3.1 70B / Llama 3.3 70B** (very good quality)
- **Qwen 2.5 72B** (excellent, multilingual)
- **Mistral Large 2** (high quality)

**Hardware requirements:**

|===
| Model       | VRAM    | GPU        | RAM 

| 70B models  | 48-80 GB| A100/H100  | 128 GB 
| 8B models   | 8-16 GB | RTX 4060 Ti| 16 GB 
|===

**Setup example with Ollama:**

```bash
#Ollama installieren
curl -fsSL https://ollama.ai/install.sh | sh
#Modell herunterladen
ollama pull llama3.1:70b
#API läuft auf http://localhost:11434/v1 (OpenAI-kompatibel)
```

===== Sizing Token Rate Limits

**Rules of thumb for TPM (Tokens per Minute):**

|===
| User scenario      | Recommended TPM    | Reason

| 1-5 users (test)   | 30,000 - 50,000   | Sufficient for testing
| 10-25 users        | 80,000 - 120,000  | Moderate usage
| 25-50 users        | 150,000 - 250,000 | Production environment
| 50+ users          | 300,000+          | High load
|===

**Customer responsibility:**

- **Azure OpenAI:** Adjust token limits in Azure AI Foundry
- **AWS Bedrock:** Check service quotas in AWS Console
- **Local models:** Ensure sufficient GPU capacity

===== Cost Overview LLM Providers

**Azure OpenAI (approx., as of 2026):**

- GPT-4o: $5-15 per 1M tokens
- GPT-4o-mini: $0.15-0.60 per 1M tokens
- **Typical (50 users): $100-500/month**

**AWS Bedrock (approx., as of 2026):**

- Claude 3.5 Sonnet: $3-8 per 1M input, $15-24 per 1M output
- Claude 3 Haiku: $0.25-1 per 1M input, $1.25-5 per 1M output
- **Typical (50 users): $80-400/month**

**Local models:**

- No API costs
- Hardware investment: €5,000-50,000+
- Ongoing electricity costs

