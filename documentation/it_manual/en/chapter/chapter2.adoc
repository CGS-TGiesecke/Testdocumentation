=== System Requirements and Dependencies

==== Hardware Requirements

The following minimum requirements apply for the productive operation of the {application}:

===== Entry Level (RAG, 8 GB RAM)

For small data volumes and low to moderate user numbers:

* Document collection up to approx. 100 MB
* Low to moderate concurrent users

|===
| *Memory (vRAM)*            | 8 GB
| *Processor (vCPU)*         | 4 vCPUs
| *Storage*                  | 250 GB SSD (virtual NVMe/SSD storage)
|===

===== Mid-Tier Level (RAG, 32 GB RAM)

For growing data volumes and higher user numbers:

* Document collection up to approx. 5 GB
* Moderate to high concurrent user numbers

|===
| *Memory (vRAM)*            | 32 GB
| *Processor (vCPU)*         | 8-12 vCPUs
| *Storage*                  | 1 TB SSD (virtual NVMe/SSD storage)
|===

===== High-End Level (RAG, 64 GB RAM)

For large data volumes, high user numbers, or compute-intensive tasks:

* Document collection over 5 GB
* Many concurrent users or complex, compute-intensive queries

|===
| *Memory (vRAM)*            | 64 GB
| *Processor (vCPU)*         | 16-32 vCPUs
| *Storage*                  | 2 TB SSD (virtual NVMe/SSD storage)
|===

==== General Requirements

|===
| *Infrastructure (VM or Server)*                | Linux-based server (local VM or cloud VM)
| *Operating System*                             | Linux, e.g. Ubuntu or a similar distribution
| *Docker Environment*                           | Docker Engine: Version ≥ 20.x 
|                                               | Docker Compose: Version ≥ 1.29 or Compose V2
| *CPU, RAM, Storage*                            | Depending on the document collection size and the number of users
| *Shared Directory* *(SHARED_FOLDER)*           | A writable directory on the server for data exchange
| *Internet Access / Proxy*                      | Download of Docker images
|                                               | Connection to Azure OpenAI API
|                                               | Proxy support is possible
| *Network / Ports*                              | Default access: Port 8000 (can be adjusted if required)
|===

*Proxy Support:* The solution supports operation behind a proxy. Proxy configuration can be specified in the environment variables.

*MCP Server API:* Optional, customer-specific extension of search functions for web, local client, image, video, and news searches as well as AI-powered summaries.

ifeval::[{big-output} == 1]
include::chapter2-2-o.adoc[]
endif::[]

==== Required Third-Party Components

The following third-party components are required for trouble-free operation and must be provided before installation:

**IMPORTANT:** {application} does **not provide an LLM**. The customer must set up and operate one of the following LLM providers themselves.

*Overview of Supported LLM Providers*

|===
| Provider         | Recommendation  | Advantages                              | Setup Effort

| **Azure OpenAI** 
| **Preferred** 
| Enterprise support, EU regions, easy integration 
| Medium

| **AWS Bedrock** 
| Equivalent  
| Good Claude models, AWS integration  
| Medium

| **Local models** 
| For high data protection requirements  
| Maximum data protection, no cloud  
| High (GPU hardware required) 
|===

**Choose a provider based on:**

- Existing cloud infrastructure (Azure or AWS)
- Data protection and compliance requirements
- Budget and operating model

Procurement and maintenance of third-party components are the responsibility of the IT department.

ifeval::[{big-output} == 1]
include::chapter2-3-o.adoc[]
endif::[]
